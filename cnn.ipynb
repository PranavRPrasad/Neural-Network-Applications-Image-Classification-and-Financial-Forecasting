{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ad7fbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-26 13:59:49.560169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fdae51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),  # Resize the image to a 100x100 square\n",
    "    transforms.ToTensor(),          # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize the image\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc16a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root='petimages', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "957ef529",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set, train_set = torch.utils.data.random_split(dataset, [int(0.2 * len(dataset)), len(dataset) - int(0.2 * len(dataset))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a30de20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epoch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0beb7664",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525ca4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efd08c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "\n",
    "    # there is no \"correct\" CNN model architecture for this lab, you can start with a naive model as follows:\n",
    "    # convolution -> relu -> pool -> convolution -> relu -> pool -> convolution -> relu -> pool -> linear -> relu -> linear -> relu -> linear\n",
    "    # you can try increasing number of convolution layers or try totally different model design\n",
    "    # convolution: nn.Conv2d (https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)\n",
    "    # pool: nn.MaxPool2d (https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html)\n",
    "    # linear: nn.Linear (https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        self.convolution_one = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.relu_one = nn.ReLU()\n",
    "        self.pool_one = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.convolution_two = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.relu_two = nn.ReLU()\n",
    "        self.pool_two = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.convolution_three = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu_three = nn.ReLU()\n",
    "        self.pool_three = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc_one = nn.Linear(64 * 12 * 12, 128)  # Adjust the input size based on your data\n",
    "        self.relu_four = nn.ReLU()\n",
    "        \n",
    "        self.fc_two = nn.Linear(128, 64)\n",
    "        self.relu_five = nn.ReLU()\n",
    "        \n",
    "        self.fc_three = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool_one(self.relu_one(self.convolution_one(x)))\n",
    "        x = self.pool_two(self.relu_two(self.convolution_two(x)))\n",
    "        x = self.pool_three(self.relu_three(self.convolution_three(x)))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu_four(self.fc_one(x))\n",
    "        x = self.relu_five(self.fc_two(x))\n",
    "        x = self.fc_three(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e88ffa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu' # whether your device has GPU\n",
    "cnn = CNN().to(device) # move the model to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a77396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search in official website for CrossEntropyLoss\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "912f6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try Adam optimizer (https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) with learning rate 0.0001, feel free to use other optimizer\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950b58e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (convolution_one): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu_one): ReLU()\n",
       "  (pool_one): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (convolution_two): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu_two): ReLU()\n",
       "  (pool_two): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (convolution_three): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu_three): ReLU()\n",
       "  (pool_three): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc_one): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (relu_four): ReLU()\n",
       "  (fc_two): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu_five): ReLU()\n",
       "  (fc_three): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784479b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.014\n",
      "[2,   100] loss: 0.013\n",
      "[3,   100] loss: 0.012\n",
      "[4,   100] loss: 0.010\n",
      "[5,   100] loss: 0.014\n",
      "[6,   100] loss: 0.011\n",
      "[7,   100] loss: 0.010\n",
      "[8,   100] loss: 0.009\n",
      "[9,   100] loss: 0.009\n",
      "[10,   100] loss: 0.007\n",
      "[11,   100] loss: 0.012\n",
      "[12,   100] loss: 0.011\n",
      "[13,   100] loss: 0.006\n",
      "[14,   100] loss: 0.013\n",
      "[15,   100] loss: 0.006\n",
      "[16,   100] loss: 0.004\n",
      "[17,   100] loss: 0.006\n",
      "[18,   100] loss: 0.009\n",
      "[19,   100] loss: 0.011\n",
      "[20,   100] loss: 0.005\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size): # begin with trying 10 epochs \n",
    "\n",
    "    loss = 0.0 # you can print out average loss per batch every certain batches\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs and label from dataloader\n",
    "        inputs, label = data\n",
    "        # move tensors to your current device (cpu or gpu)\n",
    "        inputs = inputs.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # zero the parameter gradients using zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward -> compute loss -> backward propogation -> optimize (see tutorial mentioned in main documentation)\n",
    "        outputs = cnn(inputs)  # Forward pass\n",
    "        loss = criterion(outputs, label)  # Compute the loss\n",
    "        loss.backward()  # Backward propagation\n",
    "        optimizer.step()  # Optimization step\n",
    "        # print some statistics\n",
    "        loss += loss.item() # add loss for current batch \n",
    "        if i % 100 == 99:    # print out average loss every 100 batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss / 100:.3f}')\n",
    "            loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ece0229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c228427",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = []\n",
    "prediction = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90baf2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (convolution_one): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu_one): ReLU()\n",
       "  (pool_one): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (convolution_two): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu_two): ReLU()\n",
       "  (pool_two): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (convolution_three): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (relu_three): ReLU()\n",
       "  (pool_three): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc_one): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (relu_four): ReLU()\n",
       "  (fc_two): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (relu_five): ReLU()\n",
       "  (fc_three): Linear(in_features=64, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.eval() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de74cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # since we're not training, we don't need to calculate the gradients for our outputs, so turn on no_grad mode\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        ground_truth += labels.tolist() # convert labels to list and append to ground_truth\n",
    "        # calculate outputs by running inputs through the network\n",
    "        outputs = cnn(inputs)\n",
    "        # the class with the highest logit is what we choose as prediction\n",
    "#         _, predicted = torch.max(...)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "#         prediction += ... # convert predicted to list and append to prediction\n",
    "        prediction += predicted.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a529ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GradeScope is chekcing for these three variables, you can use sklearn to calculate the scores\n",
    "# accuracy = accuracy_score(...)\n",
    "# recall = recall_score(...)\n",
    "# precision = precision_score(...)\n",
    "\n",
    "accuracy = accuracy_score(ground_truth, prediction)\n",
    "recall = recall_score(ground_truth, prediction, average='weighted')  # 'weighted' accounts for multi-class problems\n",
    "precision = precision_score(ground_truth, prediction, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97d38b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7775 0.7775 0.7804778461108393\n"
     ]
    }
   ],
   "source": [
    "print(accuracy, recall, precision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
